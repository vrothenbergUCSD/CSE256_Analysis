{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"winogrande\", \"winogrande_xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'option1', 'option2', 'answer'],\n",
       "    num_rows: 1267\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Sarah was a much better surgeon than Maria so _ always got the easier cases.',\n",
       " 'option1': 'Sarah',\n",
       " 'option2': 'Maria',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The following are pairs of Winograd Schema which are a fill-in-a-blank task '\n",
      " 'with binary options. The goal is to choose the right option for a given '\n",
      " 'sentence which requires commonsense reasoning. The pairs are in the form of '\n",
      " 'a statement S, choices [A] and [B], and an Answer.\\n'\n",
      " 'S: Sarah was a much better surgeon than Maria so _ always got the harder '\n",
      " 'cases.\\n'\n",
      " '[A] Sarah\\n'\n",
      " '[B] Maria\\n'\n",
      " 'Answer: [A] Sarah\\n'\n",
      " '\\n'\n",
      " 'S: They were worried the wine would ruin the bed and the blanket, but the _ '\n",
      " \"wasn't ruined.\\n\"\n",
      " '[A] blanket\\n'\n",
      " '[B] bed\\n'\n",
      " 'Answer: [B] bed\\n'\n",
      " '\\n'\n",
      " 'S: Terry tried to bake the eggplant in the toaster oven but the _ was too '\n",
      " 'big.\\n'\n",
      " '[A] eggplant\\n'\n",
      " '[B] toaster\\n'\n",
      " 'Answer: [A] eggplant\\n')\n",
      "Winogrande Test Accuracy: 53.51%\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialize the Llama 3 model using LangChain and Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Function to format the Winogrande example\n",
    "def format_example(example):\n",
    "    sentence = example['sentence']\n",
    "    option1 = example['option1']\n",
    "    option2 = example['option2']\n",
    "    answer = example['answer']\n",
    "    choices = [option1, option2]\n",
    "    correct_index = 0 if answer == \"1\" else 1\n",
    "    return sentence, choices, correct_index\n",
    "\n",
    "# Function to save predictions\n",
    "def save_predictions(file_path, predictions):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Index\", \"Predicted\", \"Correct\", \"Prompt\", \"Response\"])\n",
    "        writer.writerows(predictions)\n",
    "\n",
    "# Function to load predictions\n",
    "def load_predictions(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            return [(int(row[0]), int(row[1]), int(row[2])) for row in reader]\n",
    "    return []\n",
    "\n",
    "example_prompts = [\n",
    "    {\n",
    "        \"sentence\": \"Sarah was a much better surgeon than Maria so _ always got the harder cases.\",\n",
    "        \"option1\": \"Sarah\",\n",
    "        \"option2\": \"Maria\",\n",
    "        \"answer\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"They were worried the wine would ruin the bed and the blanket, but the _ wasn't ruined.\",\n",
    "        \"option1\": \"blanket\",\n",
    "        \"option2\": \"bed\",\n",
    "        \"answer\": \"2\"\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Terry tried to bake the eggplant in the toaster oven but the _ was too big.\",\n",
    "        \"option1\": \"eggplant\",\n",
    "        \"option2\": \"toaster\",\n",
    "        \"answer\": \"1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Create the initial 3-shot prompt\n",
    "init_prompt_starter = \"The following are pairs of Winograd Schema which are a fill-in-a-blank task with binary options. The goal is to choose the right option for a given sentence which requires commonsense reasoning. The pairs are in the form of a statement S, choices [A] and [B], and an Answer.\"\n",
    "init_prompt = init_prompt_starter\n",
    "for example in example_prompts:\n",
    "    sentence = example[\"sentence\"]\n",
    "    option1 = example[\"option1\"]\n",
    "    option2 = example[\"option2\"]\n",
    "    answer = f\"[A] {option1}\" if example[\"answer\"] == \"1\" else f\"[B] {option2}\"\n",
    "    init_prompt += f\"\\nS: {sentence}\\n\"\n",
    "    init_prompt += f\"[A] {option1}\\n[B] {option2}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "pprint(init_prompt)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(dataset, file_path='winogrande_predictions_3shot.csv'):\n",
    "    # Load existing predictions if they exist\n",
    "    predictions = load_predictions(file_path)\n",
    "    completed_indices = {idx for idx, _, _ in predictions}\n",
    "    correct = sum(1 for _, pred, label in predictions if pred == label)\n",
    "    total = len(predictions)\n",
    "    labels_list = ['A', 'B']\n",
    "\n",
    "    for idx, example in enumerate(dataset['validation']):\n",
    "        if idx in completed_indices:\n",
    "            continue  # Skip already processed examples\n",
    "\n",
    "        sentence, choices, correct_index = format_example(example)\n",
    "        clear_output(wait=True)\n",
    "        prompt = init_prompt\n",
    "        prompt += \"\\nYour task is to resolve the ambiguity in the following sentence.\\n\"\n",
    "        prompt += f\"S: {sentence}\\n\"\n",
    "        \n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"[{labels_list[i]}] {choice}\\n\"\n",
    "        prompt += \"\\nRespond only with the correct choice.\"\n",
    "        response = llm.invoke(prompt)\n",
    "        predicted_choice = extract_label(response, labels_list)\n",
    "\n",
    "        # Save the prediction\n",
    "        predictions.append((idx, predicted_choice, correct_index, prompt, response))\n",
    "        save_predictions(file_path, predictions)\n",
    "\n",
    "        status = \"INCORRECT\"\n",
    "        if predicted_choice == correct_index:\n",
    "            correct += 1\n",
    "            status = \"CORRECT\"\n",
    "        total += 1\n",
    "        pct = (correct / total) * 100\n",
    "        \n",
    "        print(f\"Iteration: {idx + 1} {status}\")\n",
    "        print(f\"Correct: {pct:.2f}%\")\n",
    "        print(f\"Response: {response}\")\n",
    "        pprint(prompt)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Function to extract the label from the model's response\n",
    "def extract_label(response, labels_list):\n",
    "    for i, label in enumerate(labels_list):\n",
    "        if f\"[{label}]\" in response:\n",
    "            return i\n",
    "    return -1  # Indicates an error or unrecognized format\n",
    "\n",
    "# Evaluate the model and print the accuracy\n",
    "accuracy = evaluate_model(dataset)\n",
    "print(f\"Winogrande Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is:\\n\\n[A] Sarah\\n\\nThis is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [A] Sarah.\\n\\nThe reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>I'd choose [A] blanket.\\n\\nThe sentence sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [A] eggplant.\\n\\nIn this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [A] Jeffrey.\\n\\nIn this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>1262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [B] Neil.\\n\\nThe sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [A] Joel.\\n\\nExplanation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>1264</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [B] Lindsey.\\n\\nThe pron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>1265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>I would choose [A] aquarium.\\n\\nThe pronoun \"w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Your task is to resolve the ambiguous pronoun ...</td>\n",
       "      <td>The correct answer is [A] black car.\\n\\nIn thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1267 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index  Predicted  Correct  \\\n",
       "0         0          0        1   \n",
       "1         1          0        0   \n",
       "2         2          0        1   \n",
       "3         3          0        0   \n",
       "4         4          0        0   \n",
       "...     ...        ...      ...   \n",
       "1262   1262          1        0   \n",
       "1263   1263          0        0   \n",
       "1264   1264          1        0   \n",
       "1265   1265          0        0   \n",
       "1266   1266          0        1   \n",
       "\n",
       "                                                 Prompt  \\\n",
       "0     Your task is to resolve the ambiguous pronoun ...   \n",
       "1     Your task is to resolve the ambiguous pronoun ...   \n",
       "2     Your task is to resolve the ambiguous pronoun ...   \n",
       "3     Your task is to resolve the ambiguous pronoun ...   \n",
       "4     Your task is to resolve the ambiguous pronoun ...   \n",
       "...                                                 ...   \n",
       "1262  Your task is to resolve the ambiguous pronoun ...   \n",
       "1263  Your task is to resolve the ambiguous pronoun ...   \n",
       "1264  Your task is to resolve the ambiguous pronoun ...   \n",
       "1265  Your task is to resolve the ambiguous pronoun ...   \n",
       "1266  Your task is to resolve the ambiguous pronoun ...   \n",
       "\n",
       "                                               Response  \n",
       "0     The correct answer is:\\n\\n[A] Sarah\\n\\nThis is...  \n",
       "1     The correct answer is [A] Sarah.\\n\\nThe reason...  \n",
       "2     I'd choose [A] blanket.\\n\\nThe sentence sugges...  \n",
       "3     The correct answer is [A] eggplant.\\n\\nIn this...  \n",
       "4     The correct answer is [A] Jeffrey.\\n\\nIn this ...  \n",
       "...                                                 ...  \n",
       "1262  The correct answer is [B] Neil.\\n\\nThe sentenc...  \n",
       "1263  The correct answer is [A] Joel.\\n\\nExplanation...  \n",
       "1264  The correct answer is [B] Lindsey.\\n\\nThe pron...  \n",
       "1265  I would choose [A] aquarium.\\n\\nThe pronoun \"w...  \n",
       "1266  The correct answer is [A] black car.\\n\\nIn thi...  \n",
       "\n",
       "[1267 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('winogrande_predictions.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Your task is to resolve the ambiguous pronoun in the following sentence.\\n'\n",
      " 'Sentence: Sarah was a much better surgeon than Maria so _ always got the '\n",
      " 'easier cases.\\n'\n",
      " 'Respond with one of the following choices:\\n'\n",
      " '[A] Sarah\\n'\n",
      " '[B] Maria\\n'\n",
      " 'Which choice resolves the ambiguity?')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(df.iloc[0]['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.69613259668509"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(df['Predicted'] == df['Correct']) / df.shape[0] * 100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted\n",
       "0    0.598264\n",
       "1    0.401736\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Predicted'].value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Correct\n",
       "1    0.504341\n",
       "0    0.495659\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Correct'].value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Accuracy: 54.70%\n",
      "Three-Shot Accuracy: 53.51%\n",
      "Zero-Shot Overall Response Length: 342.97 (std: 118.78)\n",
      "Three-Shot Overall Response Length: 9.89 (std: 1.96)\n",
      "Zero-Shot Correct Response Length: 340.13 (std: 115.25)\n",
      "Zero-Shot Incorrect Response Length: 346.41 (std: 122.91)\n",
      "Three-Shot Correct Response Length: 9.89 (std: 2.00)\n",
      "Three-Shot Incorrect Response Length: 9.90 (std: 1.92)\n",
      "Zero-Shot Predicted Distribution:\n",
      "Predicted\n",
      "0    0.598264\n",
      "1    0.401736\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Three-Shot Predicted Distribution:\n",
      "Predicted\n",
      " 0    0.507498\n",
      " 1    0.483031\n",
      "-1    0.009471\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_single_shot = pd.read_csv('winogrande_predictions.csv')\n",
    "df_three_shot = pd.read_csv('winogrande_predictions_3shot.csv')\n",
    "\n",
    "# Calculate accuracy for both approaches\n",
    "accuracy_single_shot = (df_single_shot['Predicted'] == df_single_shot['Correct']).mean() * 100\n",
    "accuracy_three_shot = (df_three_shot['Predicted'] == df_three_shot['Correct']).mean() * 100\n",
    "\n",
    "# Calculate the length of each response\n",
    "df_single_shot['Response_Length'] = df_single_shot['Response'].apply(len)\n",
    "df_three_shot['Response_Length'] = df_three_shot['Response'].apply(len)\n",
    "\n",
    "# Calculate overall average and standard deviation of response lengths for both approaches\n",
    "avg_length_single_shot = df_single_shot['Response_Length'].mean()\n",
    "std_length_single_shot = df_single_shot['Response_Length'].std()\n",
    "\n",
    "avg_length_three_shot = df_three_shot['Response_Length'].mean()\n",
    "std_length_three_shot = df_three_shot['Response_Length'].std()\n",
    "\n",
    "# Calculate statistics for correct and incorrect responses for both approaches\n",
    "def calculate_length_stats(df):\n",
    "    correct_responses = df[df['Predicted'] == df['Correct']]\n",
    "    incorrect_responses = df[df['Predicted'] != df['Correct']]\n",
    "    \n",
    "    stats = {\n",
    "        'avg_length_correct': correct_responses['Response_Length'].mean(),\n",
    "        'std_length_correct': correct_responses['Response_Length'].std(),\n",
    "        'avg_length_incorrect': incorrect_responses['Response_Length'].mean(),\n",
    "        'std_length_incorrect': incorrect_responses['Response_Length'].std()\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "stats_single_shot = calculate_length_stats(df_single_shot)\n",
    "stats_three_shot = calculate_length_stats(df_three_shot)\n",
    "\n",
    "# Compare the distribution of predicted labels for both approaches\n",
    "distribution_single_shot = df_single_shot['Predicted'].value_counts(normalize=True)\n",
    "distribution_three_shot = df_three_shot['Predicted'].value_counts(normalize=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Zero-Shot Accuracy: {accuracy_single_shot:.2f}%\")\n",
    "print(f\"Three-Shot Accuracy: {accuracy_three_shot:.2f}%\")\n",
    "\n",
    "print(f\"Zero-Shot Overall Response Length: {avg_length_single_shot:.2f} (std: {std_length_single_shot:.2f})\")\n",
    "print(f\"Three-Shot Overall Response Length: {avg_length_three_shot:.2f} (std: {std_length_three_shot:.2f})\")\n",
    "\n",
    "print(f\"Zero-Shot Correct Response Length: {stats_single_shot['avg_length_correct']:.2f} (std: {stats_single_shot['std_length_correct']:.2f})\")\n",
    "print(f\"Zero-Shot Incorrect Response Length: {stats_single_shot['avg_length_incorrect']:.2f} (std: {stats_single_shot['std_length_incorrect']:.2f})\")\n",
    "\n",
    "print(f\"Three-Shot Correct Response Length: {stats_three_shot['avg_length_correct']:.2f} (std: {stats_three_shot['std_length_correct']:.2f})\")\n",
    "print(f\"Three-Shot Incorrect Response Length: {stats_three_shot['avg_length_incorrect']:.2f} (std: {stats_three_shot['std_length_incorrect']:.2f})\")\n",
    "\n",
    "print(\"Zero-Shot Predicted Distribution:\")\n",
    "print(distribution_single_shot)\n",
    "\n",
    "print(\"\\nThree-Shot Predicted Distribution:\")\n",
    "print(distribution_three_shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your task is to resolve the ambiguous pronoun in the following sentence.\\nSentence: Sarah was a much better surgeon than Maria so _ always got the easier cases.\\nRespond with one of the following choices:\\n[A] Sarah\\n[B] Maria\\nWhich choice resolves the ambiguity?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_shot.iloc[0]['Prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The following are pairs of Winograd Schema which are a fill-in-a-blank task with binary options. The goal is to choose the right option for a given sentence which requires commonsense reasoning. The pairs are in the form of a statement S, choices [A] and [B], and an Answer.\n",
      "S: Sarah was a much better surgeon than Maria so _ always got the harder cases.\n",
      "[A] Sarah\n",
      "[B] Maria\n",
      "Answer: [A] Sarah\n",
      "\n",
      "S: They were worried the wine would ruin the bed and the blanket, but the _ wasn't ruined.\n",
      "[A] blanket\n",
      "[B] bed\n",
      "Answer: [B] bed\n",
      "\n",
      "S: Terry tried to bake the eggplant in the toaster oven but the _ was too big.\n",
      "[A] eggplant\n",
      "[B] toaster\n",
      "Answer: [A] eggplant\n",
      "\n",
      "Your task is to resolve the ambiguity in the following sentence.\n",
      "S: The teen found the new hat was no substitute for his cool shirt.  The _ was just cool to wear to school.\n",
      "[A] hat\n",
      "[B] shirt\n",
      "\n",
      "Respond only with the correct choice.\n",
      "Response: [A] hat\n",
      "Correct: [B]\n"
     ]
    }
   ],
   "source": [
    "def display_response(i):\n",
    "    row = df_three_shot.iloc[i]\n",
    "    answers = ['[A]', '[B]']\n",
    "    predicted = answers[row['Predicted']]\n",
    "    correct = answers[row['Correct']]\n",
    "    prompt = row['Prompt']\n",
    "    response = row['Response']\n",
    "    print('Prompt: ' + prompt)\n",
    "    print('Response: ' + response)\n",
    "    print('Correct: ' + correct)\n",
    "\n",
    "display_response(75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse251u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
